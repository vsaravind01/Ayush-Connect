input {
  file {
    #https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html
    #default is TAIL which assumes more data will come into the file.
    #change to mode => "read" if the file is a compelte file.  by default, the file will be removed once reading is complete -- backup your files if you need them.
    path => "/usr/share/logstash/ingest_data/base_dataset_v2.csv"
    start_position =>"beginning"
    sincedb_path => "/usr/share/logstash/data/plugins/inputs/file/.sincedb"
  }
}

filter {
    csv {
        separator => ","
        columns => ["key","scientific_name","accepted_scientific_name","kingdom","phylum","order","family","genus","species","generic_name","specific_epithet","taxon_rank","taxonomic_status","iucn_red_list_category","date_identified","eventDate","decimal_latitude","decimal_longitude","locality","state_province","country","continent","media", "level0","level1","level2","level3"]
    }
    # convert single quoted json field media to double quoted json field media
    mutate {
        gsub => [
            "media", "'", '"'
        ]
    }

    json {
        source => "media"
        target => "media"
        remove_field => ["message", "event"]
    }
}

filter {
    mutate {
        convert => {
            "decimal_latitude" => "float"
            "decimal_longitude" => "float"
        }
    }
}

output {
  elasticsearch {
    index => "plants"
    hosts=> ["http://elasticsearch:9200"]
  }
}